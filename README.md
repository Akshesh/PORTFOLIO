# Instruction to run app.

1. Build the Docker image by running the following command in the terminal from the root directory of the app

   ```bash
   docker build -t portfolio-app .
   ```

   This will create a Docker image with the name "portfolio-app".
2. Run the Docker container by running the following command in the terminal

   ```bash
   docker run -p 5000:5000 -v "$(pwd)/portfolio_scraper:/app/portfolio_scraper" portfolio-app
   ```

   This will start the Docker container and map port 5000 on the host machine to port 5000 on the container. It also mounts the portfolio_scraper directory from the host machine to the container so that the JSON file generated by the scraper is available to the Flask app.
3. Open a web browser and go to http://localhost:5000/ to access the app.
4. API Endpoints:
    1. Here is an example to fetch company containing substring (autocomplete)
    ```bash
    curl --location --request GET 'http://0.0.0.0:5000/autocomplete?query=1p'
    ```
    2. Here is an example to fetch all companies.
    ```bash
    curl --location --request GET 'http://0.0.0.0:5000/autocomplete'
    curl --location --request GET 'http://0.0.0.0:5000/autocomplete?query='
    ```
    3. If you want updated scrapy results, then execute following command
    ```bash
    curl --location --request GET 'http://0.0.0.0:5000/reload'
    ```

## portfolio_spider.py

### Assumptions

The PortfolioSpider class assumes that there are two websites to scrape - 'https://www.amplifypartners.com/portfolio' and 'https://lsvp.com/portfolio/'. It further assumes that the HTML structure of these websites will remain consistent, and the code will work as expected if the structure changes.

### Architecture

The PortfolioSpider class is built using the Scrapy web crawling framework. It starts by defining the name and start URLs for the spider. It then defines a parse() method that extracts the list of domains for all the portfolio companies for each website. The spider first determines which website the response is for, then extracts the relevant information using CSS selectors and stores it in a dictionary.

### Data structures

The PortfolioSpider class uses a Python dictionary to store the list of domains for all the portfolio companies for each website. The dictionary has the URL of the website as the key and a list of dictionaries as the value. Each dictionary in the list represents a company and contains its name and domain.

### Testing methodology

To test the PortfolioSpider class, we can create unit tests that check if the spider is able to extract the list of domains for all the portfolio companies for each website. The unit tests can use Scrapy's built-in test classes to simulate HTTP responses and check if the spider extracts the expected information. We can also manually test the spider by running it and inspecting the output to ensure that it matches the expected output.

## app.py

### Assumptions

The app assumes that there is a Scrapy spider named 'portfolio' that scrapes the list of domains for all the portfolio companies for the two websites defined in the spider. It further assumes that the spider outputs the list of domains in JSON format and saves it to a file named 'portfolio.json' in the 'portfolio_scraper' directory.

### Architecture

The app is built using the Flask web framework. It defines three endpoints - '/autocomplete', '/reload', and '/industry'. The '/autocomplete' endpoint takes a query parameter and returns a JSON object containing a list of companies that match the query. The '/reload' endpoint runs the Scrapy spider to update the list of domains for all the portfolio companies. The '/industry' endpoint is not implemented.

The app uses the load_data() function to read the JSON file containing the list of domains and converts it to a Python object. It then uses this object to generate the response for the '/autocomplete' endpoint.

### Data structures

The app uses a Python dictionary to store the list of companies that match the query for each URL. The dictionary has the URL as the key and a list of dictionaries as the value. Each dictionary in the list represents a company and contains its name and domain.

### Testing methodology

To test the app, we can create unit tests that simulate HTTP requests and check if the app returns the expected responses. We can also manually test the app by running it and sending HTTP requests using a tool like Postman to ensure that it behaves as expected.
